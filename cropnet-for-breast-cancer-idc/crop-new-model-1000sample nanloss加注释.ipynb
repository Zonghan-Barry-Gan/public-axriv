{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#shutil.rmtree('./data')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T17:59:54.500109Z","iopub.execute_input":"2022-05-14T17:59:54.500373Z","iopub.status.idle":"2022-05-14T17:59:54.504760Z","shell.execute_reply.started":"2022-05-14T17:59:54.500346Z","shell.execute_reply":"2022-05-14T17:59:54.503844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run of Cropnet new attention model applying on breast cancer dataset\ndataset: Breast Histopathology Images. \nhttps://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images?datasetId=7415. \nModel: A 2nd version of light attention model designed for crop disease identification. \nfrom paper: \nAutomatic identification of commodity label images using lightweight attention network, \nfrom Group of Prof Defu Zhang @ Xiamen University. DOI:doi.org/10.1007/s00521-021-06081-9","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-14T17:59:54.538467Z","iopub.execute_input":"2022-05-14T17:59:54.539312Z","iopub.status.idle":"2022-05-14T17:59:54.542871Z","shell.execute_reply.started":"2022-05-14T17:59:54.539253Z","shell.execute_reply":"2022-05-14T17:59:54.541858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##在kaggle中建立工作目录\nimport os\nos.makedirs('../working/data/train_seg/idc-minus/')     \nos.makedirs('../working/data/train_seg/idc-plus/')  \nos.makedirs('../working/data/test_seg/idc-minus/')     \nos.makedirs('../working/data/test_seg/idc-plus/')  \nos.makedirs('../working/data/val_seg/idc-minus/')     \nos.makedirs('../working/data/val_seg/idc-plus/')  \n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T17:59:54.577571Z","iopub.execute_input":"2022-05-14T17:59:54.577901Z","iopub.status.idle":"2022-05-14T17:59:54.584054Z","shell.execute_reply.started":"2022-05-14T17:59:54.577849Z","shell.execute_reply":"2022-05-14T17:59:54.582955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##全局变量声明\nsampling_seed=0\nsize_4_training=1000\nimg_x=224\nepoch_4_test=30\nn_of_classes=2\n#training_reshape=(-1, img_size, img_size, 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:01:02.693776Z","iopub.execute_input":"2022-05-14T20:01:02.694050Z","iopub.status.idle":"2022-05-14T20:01:02.698365Z","shell.execute_reply.started":"2022-05-14T20:01:02.694022Z","shell.execute_reply":"2022-05-14T20:01:02.697518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-14T17:59:54.675477Z","iopub.execute_input":"2022-05-14T17:59:54.675808Z","iopub.status.idle":"2022-05-14T17:59:54.681984Z","shell.execute_reply.started":"2022-05-14T17:59:54.675762Z","shell.execute_reply":"2022-05-14T17:59:54.681316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-14T17:59:54.715261Z","iopub.execute_input":"2022-05-14T17:59:54.715672Z","iopub.status.idle":"2022-05-14T17:59:54.721543Z","shell.execute_reply.started":"2022-05-14T17:59:54.715642Z","shell.execute_reply":"2022-05-14T17:59:54.720562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom sklearn.model_selection import train_test_split\n\nimport shutil\nfrom glob import glob \n#make directory for labelling\n\ntrain_dir='../working/data/train_seg/'\nvalidation_dir='../working/data/val_seg/'\n#train_dir='../working/data/train_seg/'  \n#test_dir = '../working/data/test_seg/normal'  \ntest_dir = '../working/data/test_seg/'  \n\nclass0 = [] # 0 = idc+\nclass1 = [] # 1 = idc-\nimagePatches = glob('../input/breast-histopathology-images/IDC_regular_ps50_idx5/**/*.png', recursive=True)\nfor filename in imagePatches:\n    if filename.endswith(\"class0.png\"):\n         class0.append(filename)\n    else:\n        class1.append(filename)\n\nprint(class0[0:10])\n\n#sampling 10000 images from class 0 and class 1 to train the model\n##从每类文件名列表中抽样\nrandom.seed(sampling_seed)\nclass0sample=random.sample(class0,size_4_training)\nclass0label=np.zeros(size_4_training)\nclass1sample=random.sample(class1,size_4_training)\nclass1label=np.ones(size_4_training)\n##从每类样本列表中分割train-val-test\nclass0sample_train, class0sample_test1, class0label_train, class0label_test1 = train_test_split(class0sample, class0label, test_size=0.3, random_state=42)\nclass0sample_test,class0sample_val,  class0label_test, class0label_val = train_test_split(class0sample_test1, class0label_test1, test_size=0.3, random_state=42)\nprint(len(class0sample_train))\nprint(len(class0sample_test))\nprint(len(class0sample_val))\nclass1sample_train, class1sample_test1, class1label_train, class1label_test1 = train_test_split(class1sample, class1label, test_size=0.3, random_state=42)\nclass1sample_test, class1sample_val, class1label_test, class1label_val = train_test_split(class1sample_test1, class1label_test1, test_size=0.3, random_state=42)\nprint(len(class1sample_train))\nprint(len(class1sample_test))\nprint(len(class1sample_val))\n##函数功能：将之前文件列表中的文件复制到指定工作目录中\ndef read_and_save_data(path, file_name_array):\n    j=0\n    for i in file_name_array:\n        if i.endswith('.png'):\n          \n            \n            #second copy method\n            head, tail = os.path.split(i)\n            outputname=str(path+tail)\n            #outputname=str(path+str(j)+'.png')\n            print(outputname)\n            shutil.copy(i, outputname)\n   \n            #print(status2)\n            \n            j=j+1\n            if j==120000:\n                break\n            \n##分别按照每类，train， validation， test保存样本   \nclass0train_path='../working/data/train_seg/idc-minus/'\nclass1train_path='../working/data/train_seg/idc-plus/'\nclass0test_path='../working/data/test_seg/idc-minus/'\nclass1test_path='../working/data/test_seg/idc-plus/'\nclass0val_path='../working/data/val_seg/idc-minus/'\nclass1val_path='../working/data/val_seg/idc-plus/'\n\nread_and_save_data(class0train_path,class0sample_train)\nread_and_save_data(class1train_path,class1sample_train)\n\n\nread_and_save_data(class0test_path,class0sample_test)\nread_and_save_data(class1test_path,class1sample_test)\n\n\nread_and_save_data(class0val_path,class0sample_val)\nread_and_save_data(class1val_path,class1sample_val)\n##print输出包含两类train，valid，test的大小和每次转存的文件用于检查","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:01:11.318307Z","iopub.execute_input":"2022-05-14T20:01:11.318912Z","iopub.status.idle":"2022-05-14T20:03:00.357772Z","shell.execute_reply.started":"2022-05-14T20:01:11.318862Z","shell.execute_reply":"2022-05-14T20:03:00.356796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shutil.rmtree('./data')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:03:12.266296Z","iopub.execute_input":"2022-05-14T18:03:12.266495Z","iopub.status.idle":"2022-05-14T18:03:12.269960Z","shell.execute_reply.started":"2022-05-14T18:03:12.266470Z","shell.execute_reply":"2022-05-14T18:03:12.269125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**load gen**","metadata":{}},{"cell_type":"code","source":"\nimport os #Operating System\nimport sys #System\n# train_generator = train_datagen.flow(x_train, y_train, batch_size =)\n# val_generator = val_datagen.flow(x_val, y_val, batch_size = 64)\n# test_generator=test_datagen.flow(x_test,y_test,batch_size = 64)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:03:33.873079Z","iopub.execute_input":"2022-05-14T20:03:33.873371Z","iopub.status.idle":"2022-05-14T20:03:33.878069Z","shell.execute_reply.started":"2022-05-14T20:03:33.873338Z","shell.execute_reply":"2022-05-14T20:03:33.877132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##这个模块（cell）把之前存好的每类train-valid-test文件分别读到np。array里\n##这个模块在其他模型上进行过测试，也没有问题\nimport os\nimport cv2\n##函数功能，将指定目录path下的文件读到列表中，并按给定的tag创建\n#输入path目标目录，tag目录内所有文件的label（目录内全部一致）\n#输出image_data，进行过resize的图片数据array\n#输出label，标签列表，列表内每一个数字都是tag，列表大小等于目录内图片数量\ndef readImage(path, tag):\n    j=0\n    image_data = []\n    label=[]\n    for i in os.listdir(path):\n        imgname=path+i\n        #print(imgname)\n        img = cv2.imread(imgname, cv2.IMREAD_COLOR)\n        img_resized = cv2.resize(img, (img_x,img_x), interpolation=cv2.INTER_LINEAR)\n        image_data.append(img_resized)\n        label.append(tag)\n        #print(img[1])\n        j=j+1\n        #if j==10:\n        #    break\n        \n    return image_data, label\n\nimport numpy as np\nfrom tensorflow.keras.utils import *\nfrom sklearn.utils import shuffle\n\nclass0_train, train0_label = readImage(class0train_path, 0)\nclass1_train, train1_label  = readImage(class1train_path, 1)\nclass0_test, test0_label = readImage(class0test_path, 0)\nclass1_test, test1_label = readImage(class1test_path, 1)\nclass0_val, val0_label = readImage(class0val_path, 0)\nclass1_val, val1_label = readImage(class1val_path, 1)\n##函数功能，将每类train的图片数据列表和标签列表进行混合并洗牌，或者是validation，或者是test\n#class0array，class1array，是train（或valid，或test） 中class0，class1的图片数据列表，label0，label1是标签列表，比如说class0是一个（700，224，224，3）的数据array，700就是图片张数，224是xy尺寸，3是三原色对应的通道。label0就是长度为700，每位都是0的array\n#输出中combined_data, combined_label分别是混合并洗牌后的数据和标签。混合通过np。concatenate，洗牌通过shuffle函数，并且combined_label标签进行了hot-coded二进制转换,通过to_categorical函数实现\n#函数中包含print检查功能，先显示combined_label的array 的shape，再整个print combined_label，以检查label是否成功二进制化，是否完全平衡\n#输出label，标签列表，列表内每一个数字都是tag，列表大小等于目录内图片数量\ndef Image_array_process(class0array,label0, class1array, label1):\n    class0_array=np.array(class0array)\n    class1_array=np.array(class1array)\n    combined_data = np.concatenate((class0_array, class1_array))\n    combined_label= np.concatenate((label0,label1), axis=0)\n    assert len(combined_data) == len(combined_label)\n    combined_data, combined_label = shuffle(combined_data, combined_label, random_state=0)\n    print(combined_data.shape)\n    length=len(combined_data)\n    #combined_label=to_categorical(combined_label,num_classes=2)\n    #combined_label=np.array(combined_label,dtype=np.int64)\n    #combined_data=np.array(combined_data,dtype=np.int64)\n    i=0\n    for i in range(length):\n        print(combined_label[i])\n\n    print\n\n    #print(class0_array.shape)\n    #print(combined_data.shape)\n    '''\n    training_reshape=(img_x,img_x,3)\n    length=len(combined_data)\n    print(length)\n    x =[None]*length\n    #print(img_data.type)\n    y =np.zeros(length)\n    i=0\n   \n    for features,label in combined_data:\n        x[i]=features\n        #print(x.shape)\n        y[i]=label\n        #print(y[i])\n        i=i+1\n    \n            #x = np.array(x).reshape(training_reshape)\n    x = np.array(x)    \n    #print(x.shape)\n    #y=np.array(y)\n    y=y.astype(int)\n    y = to_categorical(y)\n    print(y)\n    '''  \n    return combined_data, combined_label\n\n\nX_train, y_train=Image_array_process(class0_train, train0_label, class1_train, train1_label)\nX_test, y_test=Image_array_process(class0_test, test0_label, class1_test, test1_label)\nX_val, y_val=Image_array_process(class0_val, val0_label, class1_val, val1_label)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:03:38.808154Z","iopub.execute_input":"2022-05-14T20:03:38.808505Z","iopub.status.idle":"2022-05-14T20:03:40.436365Z","shell.execute_reply.started":"2022-05-14T20:03:38.808466Z","shell.execute_reply":"2022-05-14T20:03:40.435215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=X_train\ntrain_label=y_train\n\ntest_data=X_test\ntest_label=y_test\n\nval_data=X_val\nval_label=y_val","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:05:27.705984Z","iopub.execute_input":"2022-05-14T20:05:27.707043Z","iopub.status.idle":"2022-05-14T20:05:27.711845Z","shell.execute_reply.started":"2022-05-14T20:05:27.706993Z","shell.execute_reply":"2022-05-14T20:05:27.710684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:05:45.923022Z","iopub.execute_input":"2022-05-14T20:05:45.923310Z","iopub.status.idle":"2022-05-14T20:05:45.928760Z","shell.execute_reply.started":"2022-05-14T20:05:45.923275Z","shell.execute_reply":"2022-05-14T20:05:45.927694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\ntrain_dir='../working/data/train_seg/'  \n#test_dir = '../working/data/test_seg/idc-minus'  \ntest_minus_dir = '../working/data/test_seg/idc-minus' \nimg_size = (img_x, img_x)  \nepochs = epoch_4_test\nMODEL_PATH = '../working/log/tst_model.h5'\nboard_name1 = '../working/log/stage1/' + now + '/'\nboard_name2 = '../working/log/stage2/' + now + '/'\n\n#MODEL_PATH = '../working/log/tst_model.h5'\n#model.load_weights(MODEL_PATH)\n# --------------- test ----------------\ndirs = os.listdir(test_minus_dir)\n\nprob_list = []\nfor d in dirs:\n    img = image.load_img(test_minus_dir + os.sep + d, target_size=img_size)\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    y = model.predict(x)\n#    print(y)\n    #print(classes[np.argmax(y)])\n#    print(classes)\n\n    prob_list.append(str(y))   \n    file=open('../working/pred_prob.txt','w') \n    file.write('\\n'.join(prob_list))\n    file.close()\n    \n#import tensorflow as tf\n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth = True\n#keras.backend.set_session(tf.Session(config=config))\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:20:08.077994Z","iopub.execute_input":"2022-05-13T17:20:08.078324Z","iopub.status.idle":"2022-05-13T17:20:25.692710Z","shell.execute_reply.started":"2022-05-13T17:20:08.078280Z","shell.execute_reply":"2022-05-13T17:20:25.691966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PLTv3","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Jun 16 22:23:24 2020\n\n@author: HP\n\"\"\"\n'''\n\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom keras import backend as K\nfrom keras.activations import sigmoid\n\n\nfrom keras import backend as K\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nimport keras.backend.tensorflow_backend as KTF\n#import tensorflow.python.keras.backend as KTF\nimport glob\nfrom keras.layers import Input,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,concatenate,Activation,ZeroPadding2D\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import load_model\nfrom keras.layers import Activation, Dense\nfrom matplotlib import pyplot as plt\nfrom skimage import io,data\nimport time\nfrom keras import layers\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nnow = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n\nimport os,sys\nos.getcwd()\n#os.chdir(\"../working/cjd/31_CNN_Attention\")\n\n#import tensorflow as tf\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\nfrom keras import regularizers\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:20:25.693710Z","iopub.execute_input":"2022-05-13T17:20:25.693927Z","iopub.status.idle":"2022-05-13T17:20:25.701338Z","shell.execute_reply.started":"2022-05-13T17:20:25.693900Z","shell.execute_reply":"2022-05-13T17:20:25.700763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\n#import keras.backend.tensorflow_backend as KTF\nfrom tensorflow.python.keras.backend import get_session\nimport glob\nfrom keras.layers import Input,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,concatenate,Activation,ZeroPadding2D\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport keras as kr\nfrom keras.models import load_model\nfrom keras.layers import Activation, Dense\nfrom matplotlib import pyplot as plt\nfrom skimage import io,data\nimport time\n#from keras import layers\n#from keras.callbacks import TensorBoard, ModelCheckpoint\nfrom tensorflow import keras \nfrom tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nimport os,sys\n#os.getcwd()\n#os.chdir(\"/home/cjd/28_Entity_train\")\nprint(os.getcwd())\nprint (sys.version)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:03:52.177953Z","iopub.execute_input":"2022-05-14T20:03:52.179015Z","iopub.status.idle":"2022-05-14T20:03:52.206589Z","shell.execute_reply.started":"2022-05-14T20:03:52.178948Z","shell.execute_reply":"2022-05-14T20:03:52.205048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"now = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n#import os\n# \nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,5\"\n\n'''\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nkeras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n\n'''","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:03:55.624277Z","iopub.execute_input":"2022-05-14T20:03:55.624613Z","iopub.status.idle":"2022-05-14T20:03:55.633259Z","shell.execute_reply.started":"2022-05-14T20:03:55.624575Z","shell.execute_reply":"2022-05-14T20:03:55.632092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbatch_size = 32 #每批训练数据量的大小，批量多设一些；GoogleNet批量值设为20 2^n\nepochs = epoch_4_test\nMODEL_INIT = './init_model.h5'\nMODEL_PATH = './test_model.h5'\nboard_name1 = './obj_reco/stage1/' + now + '/'\nboard_name2 = './obj_reco/stage2/' + now + '/'\ntrain_dir='/data/train_seg/'\nvalidation_dir='/data/test_seg/'\nimg_size = (img_x, img_x)  # 图片大小\n#classes=list(range(1,5))\n#classes=['1','2','3','4']\nnb_train_samples = len(glob.glob(train_dir + '/*/*.*'))  # 训练样本数\nnb_validation_samples = len(glob.glob(validation_dir + '/*/*.*'))  # 验证样本数\n\n#classes = sorted([o for o in os.listdir(train_dir)])  # 根据文件名分类","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:04:00.974663Z","iopub.execute_input":"2022-05-14T20:04:00.974981Z","iopub.status.idle":"2022-05-14T20:04:00.981739Z","shell.execute_reply.started":"2022-05-14T20:04:00.974948Z","shell.execute_reply":"2022-05-14T20:04:00.980580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##添加自定义层\n#import tensorflow as tf   \n\nimport tensorflow as tf        \ndef focal_loss(gamma=2.):            \n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        return -K.sum( K.pow(1. - pt_1, gamma) * K.log(pt_1)) \n    return focal_loss_fixed\n\n\ndef Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None):  \n    if name is not None:  \n        bn_name = name + '_bn'  \n        conv_name = name + '_conv'  \n    else:  \n        bn_name = None  \n        conv_name = None  \n  \n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)  \n    x = BatchNormalization(axis=3,name=bn_name)(x)  \n    return x  \n\ndef Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False):  \n    x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same')  \n    x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same')  \n    x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same')  \n    if with_conv_shortcut:  \n        shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size)  \n        x = add([x,shortcut])  \n        return x  \n    else:  \n        x = add([x,inpt])  \n        return x  \n\n'''\n\nclass SeBlock(keras.layers.Layer):   \n    def __init__(self, reduction=4,**kwargs):\n        super(SeBlock,self).__init__(**kwargs)\n        self.reduction = reduction\n'''\nclass SeBlock(keras.layers.Layer):   \n    def __init__(self, reduction=4,**kwargs):\n        super(SeBlock,self).__init__(**kwargs)\n        self.reduction = reduction\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'reduction': self.reduction})\n        return config\n        \n    def build(self,input_shape):\n    \t#input_shape     \n    \tpass\n    def call(self, inputs):\n        x = keras.layers.GlobalAveragePooling2D()(inputs)\n        x = keras.layers.Dense(int(x.shape[-1]) // self.reduction, use_bias=False,activation=keras.activations.relu)(x)\n        x = keras.layers.Dense(int(inputs.shape[-1]), use_bias=False,activation=keras.activations.hard_sigmoid)(x)\n        return keras.layers.Multiply()([inputs,x])    \n        #return inputs*x \n\n#------#MS-DenseNet----------------------------------\ndef get_bottlenet(image_size,alpha=1.0):\n    inputs = keras.layers.Input(shape=(image_size,image_size,3),name='input_1')\n    net = keras.layers.ZeroPadding2D(padding=(3,3),name='zero_padding2d_1')(inputs)\n    net = keras.layers.Conv2D(filters=64, kernel_size=(7,7), strides=(2,2),\n                             padding='valid', name='conv1/conv')(net)\n    net = keras.layers.BatchNormalization(name='conv1/bn')(net)\n    net = keras.layers.ReLU(name='conv1/relu')(net)\n    net = keras.layers.ZeroPadding2D(padding=(1,1),name='zero_padding2d_2')(net)\n#    se = keras.layers.GlobalAveragePooling2D(name='transform2_pool')(net)\n    se=SeBlock()(net)\n\n    net = keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='valid',\n                                   name='pool1')(net)\n#    se = keras.layers.GlobalAveragePooling2D(name='transform2_pool')(net)\n    se=SeBlock()(net)\n    \n    for i in range(int(3*alpha)):\n        block = net\n        block = keras.layers.SeparableConv2D(filters=32,kernel_size=(3,3), strides=(1, 1), \n                                    padding='same',\n                                    name='conv2_block{}_2_sepconv'.format(i))(block)\n        net = keras.layers.Concatenate(name='conv2_block{}_concat'.format(i))([net,block])    \n    net = keras.layers.BatchNormalization(name='pool2_bn')(net) \n    net = keras.layers.ReLU(name='pool2_relu')(net)\n    eq = keras.layers.Dense(units=net.shape[-1],activation='sigmoid',\n                            name='transform2_dense1')(se)\n    net = keras.layers.Multiply(name='transform2_multiply')([net,eq])\n\n    net = keras.layers.Conv2D(filters=int(net.shape[-1])//2,kernel_size=(1,1),strides=(1,1),\n                              padding='same',name='pool2_conv')(net)\n    net = keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2),\n                                        name='pool2_pool')(net)\n#    se = keras.layers.GlobalAveragePooling2D(name='transform3_pool')(net)\n    se=SeBlock()(net)\n\n    for i in range(int(6*alpha)):\n        block = net\n        block = keras.layers.SeparableConv2D(filters=32, kernel_size=(3,3), strides=(1, 1),\n                                             padding='same',\n                                             name='conv3_block{}_2_sepconv'.format(i))(block)\n        net = keras.layers.Concatenate(name='conv3_block{}_concat'.format(i))([net,block])\n    net = keras.layers.BatchNormalization(name='pool3_bn')(net) \n    net = keras.layers.ReLU(name='pool3_relu')(net)\n    eq = keras.layers.Dense(units=net.shape[-1],activation='sigmoid',\n                            name='transform3_dense1')(se)\n    net = keras.layers.Multiply(name='transform3_multiply')([net,eq])\n\n    net = keras.layers.Conv2D(filters=int(net.shape[-1])//2,kernel_size=(1,1),strides=(1,1),\n                              padding='same',name='pool3_conv')(net)\n    net = keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2),\n                                    name='pool3_pool')(net)\n#    se = keras.layers.GlobalAveragePooling2D(name='transform4_pool')(net)\n    se=SeBlock()(net)\n    for i in range(int(12*alpha)):\n        block = net\n        block = keras.layers.SeparableConv2D(filters=32, kernel_size=(3,3), strides=(1, 1), \n                                    padding='same',\n                                    name='conv4_block{}_2_sepconv'.format(i))(block)\n        net = keras.layers.Concatenate(name='conv4_block{}_concat'.format(i))([net,block])\n    net = keras.layers.BatchNormalization(name='pool4_bn')(net) \n    net = keras.layers.ReLU(name='pool4_relu')(net)\n    eq = keras.layers.Dense(units=net.shape[-1],activation='sigmoid',\n                            name='transform4_dense1')(se)\n    net = keras.layers.Multiply(name='transform4_multiply')([net,eq])\n\n    net = keras.layers.Conv2D(filters=int(net.shape[-1])//2,kernel_size=(1,1),strides=(1,1),\n                             padding='same',name='pool4_conv')(net)\n    net = keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2),\n                                        name='pool4_pool')(net)\n#    se = keras.layers.GlobalAveragePooling2D(name='transform5_pool')(net)\n    se=SeBlock()(net)\n    for i in range(int(8*alpha)):\n        block = net\n        block = keras.layers.SeparableConv2D(filters=32, kernel_size=(3,3), strides=(1, 1), \n                                    padding='same',\n                                    name='conv5_block{}_2_sepconv'.format(i))(block)\n        net = keras.layers.Concatenate(name='conv5_block{}_concat'.format(i))([net,block])\n       \n    eq = keras.layers.Dense(units=net.shape[-1],activation='sigmoid',\n                            name='transform5_dense1')(se)\n    net = keras.layers.Multiply(name='transform5_multiply')([net,eq])\n    net = keras.layers.BatchNormalization(name='bn')(net)\n    net = keras.layers.ReLU(name='relu')(net)\n    model = keras.Model(inputs=inputs,outputs=net,name='mobile_densenet_bottle')\n    return model\n\n#原文中软注意力模块的组装\n\ndef get_model(image_size, alpha, classes):\n    bottlenet = get_bottlenet(alpha=alpha,image_size=image_size)\n    net = keras.layers.GlobalAveragePooling2D(name='global_pool')(bottlenet.output)\n    net = keras.layers.Dropout(rate=0.4,name='dropout1')(net)\n    net = keras.layers.Dropout(rate=0.4,name='dropout2')(net)\n    output = keras.layers.Dense(units=classes,activation='softmax',\n                             name='prediction')(net)\n    model = keras.Model(inputs=bottlenet.input,outputs=output,name='mobile_densenet')\n    return model\nmodel = get_model(image_size=img_x, alpha=1.0, classes=n_of_classes)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:04:08.515084Z","iopub.execute_input":"2022-05-14T20:04:08.515398Z","iopub.status.idle":"2022-05-14T20:04:09.342476Z","shell.execute_reply.started":"2022-05-14T20:04:08.515367Z","shell.execute_reply":"2022-05-14T20:04:09.341184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_datagen = ImageDataGenerator(validation_split=0.2)\ntrain_datagen = ImageDataGenerator()\n\ntrain_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.int64).reshape((3, 1, 1))  # remove imagenet BGR mean value\n#train_generator = train_datagen.flow(train_data, train_label, target_size=img_size, classes=classes)\ntrain_generator = train_datagen.flow(train_data, train_label)\n\nvalidation_datagen = ImageDataGenerator()\nvalidation_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.int64).reshape((3, 1, 1))\ntest_dir = '../working/data/test_seg/'  \n#val_generator = validation_datagen.flow(test_data, test_label, target_size=img_size, classes=classes)\nval_generator = validation_datagen.flow(test_data, test_label)\n\nval2_datagen = ImageDataGenerator()\nval2_datagen.mean = np.array([103.939, 116.779, 123.68], dtype=np.int64).reshape((3, 1, 1))\ntest_dir = '../working/data/test_seg/'  \n#val2_generator = val2_datagen.flow(val_data, val_label, target_size=img_size, classes=classes)\nval2_generator = val2_datagen.flow(val_data, val_label)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:06:00.042688Z","iopub.execute_input":"2022-05-14T20:06:00.043551Z","iopub.status.idle":"2022-05-14T20:06:00.659254Z","shell.execute_reply.started":"2022-05-14T20:06:00.043500Z","shell.execute_reply":"2022-05-14T20:06:00.658233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val2_generator)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:22:05.611976Z","iopub.execute_input":"2022-05-14T18:22:05.612586Z","iopub.status.idle":"2022-05-14T18:22:05.617243Z","shell.execute_reply.started":"2022-05-14T18:22:05.612546Z","shell.execute_reply":"2022-05-14T18:22:05.616288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.run_functions_eagerly(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:57:50.183984Z","iopub.execute_input":"2022-05-14T18:57:50.184253Z","iopub.status.idle":"2022-05-14T18:57:50.187946Z","shell.execute_reply.started":"2022-05-14T18:57:50.184225Z","shell.execute_reply":"2022-05-14T18:57:50.187074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nIMG_SHAPE=(img_x, img_x, 3)\nmodel_checkpoint2 = ModelCheckpoint(filepath=MODEL_PATH,  monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nboard2 = TensorBoard(log_dir=board_name2,\n                     histogram_freq=0,\n                     write_graph=True,\n                     write_images=True)\ncallback_list2 = [model_checkpoint2, board2]\nlearning_rate = 0.01\ndecay = 1e-6\nmomentum = 0.8\nnesterov = True\nsgd_optimizer = keras.optimizers.SGD(lr = learning_rate, decay = decay,            \n                    momentum = momentum, nesterov = nesterov)\nmodel.compile(loss = [focal_loss(gamma=2)], optimizer = sgd_optimizer, metrics = ['acc'])\n\nnb_train_samples = len(glob.glob(train_dir + '/*/*.*'))  \nnb_validation_samples = len(glob.glob(validation_dir + '/*/*.*'))  \n\n\nmodel.fit_generator(train_generator, steps_per_epoch=nb_train_samples //batch_size, epochs=epochs,\n#                    validation_data=val_generator, validation_steps=nb_validation_samples // batch_size,\n#                    verbose=1, shuffle=True)#callbacks=callback_list2,\n#history1=model.fit(\n#   x=X_train,\n#    y=y_train,\n#    batch_size=32,\n#    epochs=30,\n#    verbose=1,\n#    callbacks=callback_list2,\n#    validation_split=0.0,\n#    validation_data=(X_test, y_test),\n#    shuffle=True,\n#    class_weight=None,\n#    sample_weight=None,\n#    initial_epoch=0,\n#    steps_per_epoch=None,\n#    validation_steps=None,\n#    validation_batch_size=None,\n#    validation_freq=1,\n#    max_queue_size=10,\n#    workers=1,\n#    use_multiprocessing=False\n#)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:26:17.035760Z","iopub.execute_input":"2022-05-14T20:26:17.036069Z","iopub.status.idle":"2022-05-14T21:52:39.135631Z","shell.execute_reply.started":"2022-05-14T20:26:17.036036Z","shell.execute_reply":"2022-05-14T21:52:39.134707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"print(val_generator.next())","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:13:12.339632Z","iopub.execute_input":"2022-05-14T20:13:12.340191Z","iopub.status.idle":"2022-05-14T20:13:12.359986Z","shell.execute_reply.started":"2022-05-14T20:13:12.340155Z","shell.execute_reply":"2022-05-14T20:13:12.358900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.fit(\n#    train_generator,\n#    batch_size=batch_size,\n#    epochs=epoch_4_test,\n#    verbose=1,\n#    callbacks=callback_list2,\n#    validation_data=val_generator,\n#    shuffle=True,\n#    class_weight=None,\n#    sample_weight=None,\n#   initial_epoch=0,\n#   steps_per_epoch=None,\n#   validation_steps=None,\n#   validation_batch_size=None,\n#   validation_freq=1,\n#   max_queue_size=10,\n#   workers=1,\n#   use_multiprocessing=False\n#)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nnb_train_samples = len(train_label)  \nnb_validation_samples = len(test_label)  \nprint(nb_train_samples //batch_size)\nprint(nb_validation_samples //batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:48:49.145758Z","iopub.execute_input":"2022-05-14T18:48:49.146057Z","iopub.status.idle":"2022-05-14T18:48:49.154978Z","shell.execute_reply.started":"2022-05-14T18:48:49.146026Z","shell.execute_reply":"2022-05-14T18:48:49.154112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val_generator.next())","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:39:38.918678Z","iopub.execute_input":"2022-05-14T18:39:38.918996Z","iopub.status.idle":"2022-05-14T18:39:38.939594Z","shell.execute_reply.started":"2022-05-14T18:39:38.918962Z","shell.execute_reply":"2022-05-14T18:39:38.938799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#显示第一次训练过程中accuracy和loss的变化\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (12, 6))\nplt.subplot(121)\n#plotting the Accuracy of test and training sets\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('1st training Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(122)\n#plotting the loss of test and training sets\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('1st training Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T22:37:04.484765Z","iopub.execute_input":"2022-05-14T22:37:04.485053Z","iopub.status.idle":"2022-05-14T22:37:04.786502Z","shell.execute_reply.started":"2022-05-14T22:37:04.485025Z","shell.execute_reply":"2022-05-14T22:37:04.785542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score, accuracy_score, confusion_matrix\n\ndef get_accuracy_metrics(model, X_train, y_train, X_val, y_val, X_test, y_test):\n    print(\"Train accuracy Score------------>\")\n    print (\"{0:.3f}\".format(accuracy_score(y_train, model.predict(X_train))*100), \"%\")\n    \n    print(\"Val accuracy Score--------->\")\n    val_pred = model.predict(X_val)\n    print(\"{0:.3f}\".format(accuracy_score(y_val, val_pred)*100), \"%\")\n    \n    predicted =  model.predict(X_test)\n    print(\"Test accuracy Score--------->\")\n    print(\"{0:.3f}\".format(accuracy_score(y_test, predicted)*100), \"%\")\n    \n    print(\"F1 Score--------------->\")\n    print(\"{0:.3f}\".format(f1_score(y_test, predicted, average = 'weighted')*100), \"%\")\n    \n    print(\"Cohen Kappa Score------------->\")\n    print(\"{0:.3f}\".format(cohen_kappa_score(y_test, predicted)*100), \"%\")\n    \n    print(\"Recall-------------->\")\n    print(\"{0:.3f}\".format(recall_score(y_test, predicted, average = 'weighted')*100), \"%\")\n    \n    print(\"Precision-------------->\")\n    print(\"{0:.3f}\".format(precision_score(y_test, predicted, average = 'weighted')*100), \"%\")\n    \n    cf_matrix_test = confusion_matrix(y_test, predicted)\n    cf_matrix_val = confusion_matrix(y_val, val_pred)\n    \n    plt.figure(figsize = (12, 6))\n    plt.subplot(121)\n    sns.heatmap(cf_matrix_val, annot=True, cmap='Blues')\n    plt.title(\"Val Confusion matrix\")\n    \n    plt.subplot(122)\n    sns.heatmap(cf_matrix_test, annot=True, cmap='Blues')\n    plt.title(\"Test Confusion matrix\")\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T22:32:32.050428Z","iopub.execute_input":"2022-05-14T22:32:32.051636Z","iopub.status.idle":"2022-05-14T22:32:32.267308Z","shell.execute_reply.started":"2022-05-14T22:32:32.051572Z","shell.execute_reply":"2022-05-14T22:32:32.266432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-14T22:33:10.406823Z","iopub.execute_input":"2022-05-14T22:33:10.407115Z","iopub.status.idle":"2022-05-14T22:33:51.571438Z","shell.execute_reply.started":"2022-05-14T22:33:10.407086Z","shell.execute_reply":"2022-05-14T22:33:51.570071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定义评分函数\nimport seaborn as sns\nfrom sklearn.metrics import f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score, accuracy_score, confusion_matrix\n\ndef test_2_final_model(model, train_generator, test_generator, val_generator, y_train, y_test, y_val, class_labels):\n    \n    # BS = 16\n    ##results = dict()\n    \n    # n = len(testy)// BS\n\n    # testX = testX[:BS*n]\n    # testy = testy[:BS*n]\n\n    ##print('Predicting test data')\n    ##test_start_time = datetime.now()\n    y_pred_test_original = model.predict_generator(test_generator,verbose=1)\n    # y_pred = (y_pred_test>0.5).astype('int')\n\n    y_pred_test = np.argmax(y_pred_test_original, axis = 1)\n \n    y_test = y_test.astype(int) # sparse form not categorical\n    y_train = y_train.astype(int) # sparse form not categorical\n    y_val = y_val.astype(int) # sparse form not categorical\n    # y_test = np.argmax(testy, axis= 1)\n    #y_test = np.argmax(testy, axis=-1)\n    \n    ##test_end_time = datetime.now()\n    ##print('Done \\n \\n')\n    ##results['testing_time'] = test_end_time - test_start_time\n    ##print('testing time(HH:MM:SS:ms) - {}\\n\\n'.format(results['testing_time']))\n    ##results['predicted'] = y_pred_test\n\n\n    # balanced_accuracy\n    from sklearn.metrics import balanced_accuracy_score\n    ##balanced_accuracy = balanced_accuracy_score(y_true=y_test, y_pred=y_pred_test)\n    ##print('---------------------')\n    ##print('| Balanced Accuracy  |')\n    ##print('---------------------')\n    ##print('\\n    {}\\n\\n'.format(balanced_accuracy))\n\n    \n    # calculate overall accuracty of the model\n    ##accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred_test)\n    # store accuracy in results\n    ##results['accuracy'] = accuracy\n    ##print('---------------------')\n    ##print('|      Accuracy      |')\n    ##print('---------------------')\n    ##print('\\n    {}\\n\\n'.format(accuracy))\n    \n\n    # get classification report\n    ##print('-------------------------')\n    ##print('| Classifiction Report |')\n    ##print('-------------------------')\n    ##classification_report = metrics.classification_report(y_test, y_pred_test)\n    # store report in results\n    ##results['classification_report'] = classification_report\n    ##print(classification_report)\n    \n    \n    \n    # confusion matrix\n    ##cm = metrics.confusion_matrix(y_test, y_pred_test)\n    ##results['confusion_matrix'] = cm\n    ##if print_cm: \n    ##    print('--------------------')\n    ##    print('| Confusion Matrix |')\n    ##    print('--------------------')\n    ##    print('\\n {}'.format(cm))\n        \n    # plot confusin matrix\n    ##plt.figure(figsize=(6,4))\n    ##plt.grid(b=False)\n    ##plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized confusion matrix')\n    ##plt.show()\n    \n\n    \n    # add the trained  model to the results\n    ##results['model'] = model\n    ##print(\"calculate other score\")\n    ##print(\"predicting validation data\")\n\n    #calculate other score\n    y_pred_val_original = model.predict_generator(val_generator,verbose=1)\n    # y_pred = (y_pred_val>0.5).astype('int')\n\n    y_pred_val = np.argmax(y_pred_val_original, axis = 1)\n    # y_val = np.argmax(valy, axis= 1)\n    #y_val = np.argmax(valy, axis=-1)\n    print(\"predicting test data\")\n    y_pred_train_original = model.predict_generator(train_generator,verbose=1)\n    # y_pred = (y_pred_train>0.5).astype('int')\n\n    y_pred_train = np.argmax(y_pred_train_original, axis = 1)\n    # y_train = np.argmax(trainy, axis= 1)\n    #y_train = np.argmax(trainy, axis=-1)\n    \n    print(\"Train accuracy Score------------>\")\n    print (\"{0:.3f}\".format(accuracy_score(y_train, y_pred_train)*100), \"%\")\n    \n    print(\"Val accuracy Score--------->\")\n    \n    print(\"{0:.3f}\".format(accuracy_score(y_val, y_pred_val)*100), \"%\")\n    \n\n    \n  \n    print(\"Test accuracy Score--------->\")\n    print(\"{0:.3f}\".format(accuracy_score(y_test, y_pred_test)*100), \"%\")\n    \n    print(\"F1 Score--------------->\")\n    print(\"{0:.3f}\".format(f1_score(y_test, y_pred_test, average = 'weighted')*100), \"%\")\n    \n    print(\"Cohen Kappa Score------------->\")\n    print(\"{0:.3f}\".format(cohen_kappa_score(y_test, y_pred_test)*100), \"%\")\n    \n    print(\"Recall-------------->\")\n    print(\"{0:.3f}\".format(recall_score(y_test, y_pred_test, average = 'weighted')*100), \"%\")\n    \n    print(\"Precision-------------->\")\n    print(\"{0:.3f}\".format(precision_score(y_test, y_pred_test, average = 'weighted')*100), \"%\")\n    \n    cf_matrix_test = confusion_matrix(y_test, y_pred_test)\n    cf_matrix_val = confusion_matrix(y_val, y_pred_val)\n    \n    plt.figure(figsize = (12, 6))\n    plt.subplot(121)\n    sns.heatmap(cf_matrix_val, annot=True, cmap='Blues')\n    plt.title(\"Val Confusion matrix\")\n    \n    plt.subplot(122)\n    sns.heatmap(cf_matrix_test, annot=True, cmap='Blues')\n    plt.title(\"Test Confusion matrix\")\n    \n    plt.show()\n    \n    \n    return","metadata":{"execution":{"iopub.status.busy":"2022-05-13T19:06:32.811433Z","iopub.execute_input":"2022-05-13T19:06:32.811670Z","iopub.status.idle":"2022-05-13T19:06:32.832113Z","shell.execute_reply.started":"2022-05-13T19:06:32.811642Z","shell.execute_reply":"2022-05-13T19:06:32.831351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.metrics_names","metadata":{"execution":{"iopub.status.busy":"2022-05-14T20:25:44.575006Z","iopub.execute_input":"2022-05-14T20:25:44.575781Z","iopub.status.idle":"2022-05-14T20:25:44.582148Z","shell.execute_reply.started":"2022-05-14T20:25:44.575740Z","shell.execute_reply":"2022-05-14T20:25:44.581238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##作图显示第二次训练中accuracy和lost的变化\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (12, 6))\nplt.subplot(121)\n#plotting the Accuracy of test and training sets\nplt.plot(history2.history['acc'])\nplt.plot(history2.history['val_acc'])\nplt.title('1st training Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(122)\n#plotting the loss of test and training sets\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('1st training Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-13T19:25:02.239567Z","iopub.execute_input":"2022-05-13T19:25:02.239897Z","iopub.status.idle":"2022-05-13T19:25:03.252100Z","shell.execute_reply.started":"2022-05-13T19:25:02.239865Z","shell.execute_reply":"2022-05-13T19:25:03.251049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-13T19:25:42.709044Z","iopub.execute_input":"2022-05-13T19:25:42.709859Z","iopub.status.idle":"2022-05-13T19:25:43.149381Z","shell.execute_reply.started":"2022-05-13T19:25:42.709817Z","shell.execute_reply":"2022-05-13T19:25:43.148748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}