{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dog Memorizer GAN\n## DISCLAIMER: This may not be a valid comp solution\nThis notebook is refer to Chris Deotte's notebook on 2019 using GAN for dog images.\nreference show hear\n\"Dog Memorizer GAN\". 2019. Deotte, C. Accessed August 12 2022. https://www.kaggle.com/code/cdeotte/dog-memorizer-gan.\n\n\nIn this kernel we attempt to create a new kind of semi-supervised GAN. (Typical GAN design is presented in Nanashi's great tutorial [here][4]). Instead of having the Generator and Discriminator learn at the same time, we will first train the Discriminator to memorize all the training images. Next the Discriminator will teach the images to the Generator. We give the Generator poor memory (with a bottleneck) in hopes that it will learn to generalize. For our Kaggle submission, we will ask this Generator to output a mixture of (the generalized) images it has learned. Since the images are stored in a conv net, we hope to get a generalized conceptual mixture (versus a pixel blend). (This kernel is inspired by the tutorial [here][1] and by my previous kernels [here][2] and [here][3]).\n\n![image](http://playagricola.com/Kaggle/gan7419.png)\n\nA GAN consists of a Generator and Discriminator. After being trained, a Generator is a robot artist that draws dog images. During training the Discriminator teaches the Generator how to draw a dog. (And typically the G teaches the D to disguish real from fake dogs). The Generator never sees any images of dogs. Instead it continually attempts to draw a dog and is coached by the Discriminator. In this kernel, the **Memorizer** Generator is coached to memorize images from the training set. (We hope generalization arises from poor memory). In contrast, a **Generalizing** Generator is coached to generalize images!\n\n# Load and Crop Images\n\n[1]: https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3\n[2]: https://www.kaggle.com/cdeotte/dog-autoencoder\n[3]: https://www.kaggle.com/cdeotte/supervised-generative-dog-net\n[4]: https://www.kaggle.com/jesucristo/gan-introduction","metadata":{}},{"cell_type":"code","source":"ComputeLB = True\nDogsOnly = True\nimport sklearn\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport numpy as np, pandas as pd, os\nimport xml.etree.ElementTree as ET \nimport matplotlib.pyplot as plt, zipfile \nfrom PIL import Image \nROOT = '../input/generative-dog-images/'","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:12:35.345127Z","iopub.execute_input":"2022-08-11T19:12:35.345359Z","iopub.status.idle":"2022-08-11T19:12:36.319292Z","shell.execute_reply.started":"2022-08-11T19:12:35.345310Z","shell.execute_reply":"2022-08-11T19:12:36.318130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\nif not ComputeLB: ROOT = '../input/'\nIMAGES = os.listdir(ROOT + 'all-dogs/all-dogs/')\nbreeds = os.listdir(ROOT + 'annotation/Annotation/') \n\nidxIn = 0; namesIn = []\nimagesIn = np.zeros((25000,img_size,img_size,3))\n\n# CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n# https://www.kaggle.com/paulorzp/show-annotations-and-breeds\nif DogsOnly:\n    for breed in breeds:\n        for dog in os.listdir(ROOT+'annotation/Annotation/'+breed):\n            try: img = Image.open(ROOT+'all-dogs/all-dogs/'+dog+'.jpg') \n            except: continue           \n            tree = ET.parse(ROOT+'annotation/Annotation/'+breed+'/'+dog)\n            root = tree.getroot()\n            objects = root.findall('object')\n            for o in objects:\n                bndbox = o.find('bndbox') \n                xmin = int(bndbox.find('xmin').text)\n                ymin = int(bndbox.find('ymin').text)\n                xmax = int(bndbox.find('xmax').text)\n                ymax = int(bndbox.find('ymax').text)\n                w = np.min((xmax - xmin, ymax - ymin))\n                img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n                img2 = img2.resize((img_size,img_size), Image.ANTIALIAS)\n                imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                #if idxIn%1000==0: print(idxIn)\n                namesIn.append(breed)\n                idxIn += 1\n    idx = np.arange(idxIn)\n    np.random.shuffle(idx)\n    imagesIn = imagesIn[idx,:,:,:]\n    namesIn = np.array(namesIn)[idx]\n    \n# RANDOMLY CROP FULL IMAGES\nelse:\n    x = np.random.choice(np.arange(20579),size_discriminator)\n    for k in range(len(x)):\n        img = Image.open(ROOT + 'all-dogs/all-dogs/' + IMAGES[x[k]])\n        w = img.size[0]\n        h = img.size[1]\n        sz = np.min((w,h))\n        a=0; b=0\n        if w<h: b = (h-sz)//2\n        else: a = (w-sz)//2\n        img = img.crop((0+a, 0+b, sz+a, sz+b))  \n        img = img.resize((img_size,img_size), Image.ANTIALIAS)   \n        imagesIn[idxIn,:,:,:] = np.asarray(img)\n        namesIn.append(IMAGES[x[k]])\n        if idxIn%1000==0: print(idxIn)\n        idxIn += 1\n    \n# DISPLAY CROPPED IMAGES\nx = np.random.randint(0,idxIn,25)\nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n        plt.axis('off')\n        if not DogsOnly: plt.title(namesIn[x[k*5+j]],fontsize=11)\n        else: plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n        plt.imshow(img)\n    plt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-25T09:47:00.150130Z","iopub.execute_input":"2022-07-25T09:47:00.150565Z","iopub.status.idle":"2022-07-25T09:47:00.468011Z","shell.execute_reply.started":"2022-07-25T09:47:00.150396Z","shell.execute_reply":"2022-07-25T09:47:00.465275Z"}}},{"cell_type":"markdown","source":"# Build Discriminator","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Conv2D, Reshape, Flatten, concatenate, UpSampling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.optimizers import SGD, Adam","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:12:36.321156Z","iopub.execute_input":"2022-08-11T19:12:36.321424Z","iopub.status.idle":"2022-08-11T19:12:37.361652Z","shell.execute_reply.started":"2022-08-11T19:12:36.321377Z","shell.execute_reply":"2022-08-11T19:12:37.360900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_4_training=7142\nsize_4_training0=size_4_training\nsize_4_training1=size_4_training\nsize_4_training_total=size_4_training0+size_4_training1\nsize_discriminator=int(size_4_training_total*0.7)\nprint(size_discriminator)\nimg_size=50\nimg_size_flatten=img_size*img_size*3\nsampling_seed = 0\nepoch_4_test = 50","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:12:37.364448Z","iopub.execute_input":"2022-08-11T19:12:37.364953Z","iopub.status.idle":"2022-08-11T19:12:37.371703Z","shell.execute_reply.started":"2022-08-11T19:12:37.364897Z","shell.execute_reply":"2022-08-11T19:12:37.370573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read in Breast Cancer Images","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs('../working/data/train_seg/idc-minus/')     \nos.makedirs('../working/data/train_seg/idc-plus/')  \nos.makedirs('../working/data/test_seg/idc-minus/')     \nos.makedirs('../working/data/test_seg/idc-plus/')  \nos.makedirs('../working/data/val_seg/idc-minus/')     \nos.makedirs('../working/data/val_seg/idc-plus/') ","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:12:37.373268Z","iopub.execute_input":"2022-08-11T19:12:37.373786Z","iopub.status.idle":"2022-08-11T19:12:37.382366Z","shell.execute_reply.started":"2022-08-11T19:12:37.373516Z","shell.execute_reply":"2022-08-11T19:12:37.381645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom glob import glob\nimagePatches = glob('../input/breast-histopathology-images/IDC_regular_ps50_idx5/**/*.png', recursive=True)\ntrain_dir='../working/data/train_seg/'\nvalidation_dir='../working/data/val_seg/'\n#train_dir='../working/data/train_seg/'  \n#test_dir = '../working/data/test_seg/normal'  \ntest_dir = '../working/data/test_seg/'  \n\nclass0 = [] # 0 = idc+\nclass1 = [] # 1 = idc-\nimagePatches = glob('../input/breast-histopathology-images/IDC_regular_ps50_idx5/**/*.png', recursive=True)\nfor filename in imagePatches:\n    if filename.endswith(\"class0.png\"):\n         class0.append(filename)\n    else:\n        class1.append(filename)\n\nprint(class0[0:10])\n\n#sampling 10000 images from class 0 and class 1 to train the model\n\nrandom.seed(sampling_seed)\nclass0sample=random.sample(class0,size_4_training0)\nclass0label=np.zeros(size_4_training0)\nclass1sample=random.sample(class1,size_4_training1)\nclass1label=np.ones(size_4_training1)\n\nclass0sample_train, class0sample_test1, class0label_train, class0label_test1 = train_test_split(class0sample, class0label, test_size=0.3, random_state=42)\nclass0sample_val, class0sample_test,  class0label_val, class0label_test = train_test_split(class0sample_test1, class0label_test1, test_size=0.3, random_state=42)\nprint(len(class0sample_train))\nprint(len(class0sample_test))\nprint(len(class0sample_val))\nclass1sample_train, class1sample_test1, class1label_train, class1label_test1 = train_test_split(class1sample, class1label, test_size=0.3, random_state=42)\nclass1sample_val, class1sample_test, class1label_val, class1label_test = train_test_split(class1sample_test1, class1label_test1, test_size=0.3, random_state=42)\nprint(len(class1sample_train))\nprint(len(class1sample_test))\nprint(len(class1sample_val))\ndef read_and_save_data(path, file_name_array):\n    j=0\n    for i in file_name_array:\n        if i.endswith('.png'):\n          \n            \n            #second copy method\n            head, tail = os.path.split(i)\n            outputname=str(path+tail)\n            #outputname=str(path+str(j)+'.png')\n            #print(outputname)\n\n            shutil.copy(i, outputname)\n             \n            #print(status2)\n            \n            j=j+1\n            if j==120000:\n                break\n            \n   \nclass0train_path='../working/data/train_seg/idc-minus/'\nclass1train_path='../working/data/train_seg/idc-plus/'\nclass0test_path='../working/data/test_seg/idc-minus/'\nclass1test_path='../working/data/test_seg/idc-plus/'\nclass0val_path='../working/data/val_seg/idc-minus/'\nclass1val_path='../working/data/val_seg/idc-plus/'\n\nread_and_save_data(class0train_path,class0sample_train)\nread_and_save_data(class1train_path,class1sample_train)\n\n\nread_and_save_data(class0test_path,class0sample_test)\nread_and_save_data(class1test_path,class1sample_test)\n\n\nread_and_save_data(class0val_path,class0sample_val)\nread_and_save_data(class1val_path,class1sample_val)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:12:37.385376Z","iopub.execute_input":"2022-08-11T19:12:37.386064Z","iopub.status.idle":"2022-08-11T19:17:44.388937Z","shell.execute_reply.started":"2022-08-11T19:12:37.386007Z","shell.execute_reply":"2022-08-11T19:17:44.388145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os #Operating System\nimport sys #System\nimport cv2\ndef readImage(path, tag):\n    j=0\n    image_data = []\n    label=[]\n    for i in os.listdir(path):\n        imgname=path+i\n        #print(imgname)\n        img = cv2.imread(imgname, cv2.IMREAD_COLOR)\n        img_resized = cv2.resize(img, (img_size,img_size), interpolation=cv2.INTER_LINEAR)\n        image_data.append(img_resized)\n        label.append(tag)\n        #print(img[1])\n        j=j+1\n        #if j==10:\n        #    break\n        \n    return image_data, label\n\nimport numpy as np\nfrom tensorflow.keras.utils import *\nfrom sklearn.utils import shuffle\n\nclass0_train, train0_label = readImage(class0train_path, 0)\nclass1_train, train1_label  = readImage(class1train_path, 1)\nclass0_test, test0_label = readImage(class0test_path, 0)\nclass1_test, test1_label = readImage(class1test_path, 1)\nclass0_val, val0_label = readImage(class0val_path, 0)\nclass1_val, val1_label = readImage(class1val_path, 1)\n\ndef Image_array_process(class0array,label0, class1array, label1):\n    class0_array=np.array(class0array)\n    class1_array=np.array(class1array)\n    combined_data = np.concatenate((class0_array, class1_array))\n    combined_label= np.concatenate((label0,label1), axis=0)\n    assert len(combined_data) == len(combined_label)\n    combined_data, combined_label = shuffle(combined_data, combined_label, random_state=0)\n    print(combined_data.shape)\n    length=len(combined_data)\n    combined_label=to_categorical(combined_label,num_classes=2)\n    #i=0\n    #for i in range(length):\n    #    print(combined_label[i])\n\n    #print\n\n    #print(class0_array.shape)\n    #print(combined_data.shape)\n    '''\n    training_reshape=(224,224,3)\n    length=len(combined_data)\n    print(length)\n    x =[None]*length\n    #print(img_data.type)\n    y =np.zeros(length)\n    i=0\n   \n    for features,label in combined_data:\n        x[i]=features\n        #print(x.shape)\n        y[i]=label\n        #print(y[i])\n        i=i+1\n    \n            #x = np.array(x).reshape(training_reshape)\n    x = np.array(x)    \n    #print(x.shape)\n    #y=np.array(y)\n    y=y.astype(int)\n    y = to_categorical(y)\n    print(y)\n    '''  \n    return combined_data, combined_label\n\n\nX_train, y_train=Image_array_process(class0_train, train0_label, class1_train, train1_label)\nX_test, y_test=Image_array_process(class0_test, test0_label, class1_test, test1_label)\nX_val, y_val=Image_array_process(class0_val, val0_label, class1_val, val1_label)\n#imagesIn, y_total=Image_array_process(class0sample, class0label, class1sample, class1label)\nimagesIn=X_train","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:17:44.390520Z","iopub.execute_input":"2022-08-11T19:17:44.390827Z","iopub.status.idle":"2022-08-11T19:17:47.060993Z","shell.execute_reply.started":"2022-08-11T19:17:44.390780Z","shell.execute_reply":"2022-08-11T19:17:47.059542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_discriminator=int(size_4_training_total*0.7)\n# BUILD DISCRIMINATIVE NETWORK\ndog = Input((img_size_flatten,))\n#Explain: 12288 is the total data size of the img: img_sizeximg_sizex3=12288\ndogName = Input((size_discriminator,))\nx = Dense(img_size_flatten, activation='sigmoid')(dogName) \nx = Reshape((2,img_size_flatten,1))(concatenate([dog,x]))\nx = Conv2D(1,(2,1),use_bias=False,name='conv')(x)\ndiscriminated = Flatten()(x)\n\n# COMPILE\ndiscriminator = Model([dog,dogName], discriminated)\ndiscriminator.get_layer('conv').trainable = False\ndiscriminator.get_layer('conv').set_weights([np.array([[[[-1.0 ]]],[[[1.0]]]])])\ndiscriminator.compile(optimizer='adam', loss='binary_crossentropy')\n\n# DISPLAY ARCHITECTURE\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:17:47.063475Z","iopub.execute_input":"2022-08-11T19:17:47.064084Z","iopub.status.idle":"2022-08-11T19:17:49.019835Z","shell.execute_reply.started":"2022-08-11T19:17:47.064030Z","shell.execute_reply":"2022-08-11T19:17:49.019011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Discriminator\nWe will train the Discriminator to memorize the training images. (Typically you don't train the Discriminator ahead of time. The D learns as the G learns. But this GAN is special).","metadata":{}},{"cell_type":"code","source":"\nprint(imagesIn.shape)\n#print(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:17:49.021353Z","iopub.execute_input":"2022-08-11T19:17:49.021683Z","iopub.status.idle":"2022-08-11T19:17:49.027559Z","shell.execute_reply.started":"2022-08-11T19:17:49.021625Z","shell.execute_reply":"2022-08-11T19:17:49.026637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING DATA\n\ntrain_y = (imagesIn[:size_discriminator,:,:,:]/255.).reshape((-1,img_size_flatten))\ntrain_X = np.zeros((size_discriminator,size_discriminator))\nfor i in range(size_discriminator): train_X[i,i] = 1\nzeros = np.zeros((size_discriminator,img_size_flatten))\n\n# TRAIN NETWORK\nlr = 0.5\nfor k in range(5):\n    annealer = LearningRateScheduler(lambda x: lr)\n    h = discriminator.fit([zeros,train_X], train_y, epochs = 10, batch_size=256, callbacks=[annealer], verbose=0)\n    print('Epoch',(k+1)*10,'/30 - loss =',h.history['loss'][-1] )\n    if h.history['loss'][-1]<0.533: lr = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:17:49.028671Z","iopub.execute_input":"2022-08-11T19:17:49.028964Z","iopub.status.idle":"2022-08-11T19:19:49.799009Z","shell.execute_reply.started":"2022-08-11T19:17:49.028891Z","shell.execute_reply":"2022-08-11T19:19:49.798282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Delete Training Images\nOur Discriminator has memorized all the training images. We will now delete the training images. Our Generator will never see the training images. It will only be coached by the Discriminator. Below are examples of images that the Discriminator memorized.","metadata":{}},{"cell_type":"code","source":"del train_X, train_y, imagesIn","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:19:49.800291Z","iopub.execute_input":"2022-08-11T19:19:49.800718Z","iopub.status.idle":"2022-08-11T19:19:49.841943Z","shell.execute_reply.started":"2022-08-11T19:19:49.800547Z","shell.execute_reply":"2022-08-11T19:19:49.840989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Discriminator Recalls from Memory Dogs')    \nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        xx = np.zeros((size_discriminator))\n        xx[np.random.randint(size_discriminator)] = 1\n        plt.subplot(1,5,j+1)\n        img = discriminator.predict([zeros[0,:].reshape((-1,img_size_flatten)),xx.reshape((-1,size_discriminator))]).reshape((-1,img_size,img_size,3))\n        img = Image.fromarray( (255*img).astype('uint8').reshape((img_size,img_size,3)))\n        plt.axis('off')\n        plt.imshow(img)\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-07T12:56:12.578597Z","iopub.execute_input":"2022-08-07T12:56:12.578989Z","iopub.status.idle":"2022-08-07T12:56:15.062766Z","shell.execute_reply.started":"2022-08-07T12:56:12.578905Z","shell.execute_reply":"2022-08-07T12:56:15.061913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Generator and GAN\nWe will purposely give our Generator a bottleneck in its memory. Using poor memory forces the Generator to learn a generalization of images and not memorize the images exactly.","metadata":{}},{"cell_type":"code","source":"# BUILD GENERATOR NETWORK\nBadMemory = True\n\nif BadMemory:\n    seed = Input((size_discriminator,))\n#Q: is seed the number of image input\n    x = Dense(3200, activation='elu')(seed)\n    x = Reshape((10,10,32))(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same')(x)\n    x = UpSampling2D((5, 5))(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)\n    #x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same')(x)\n    #x = UpSampling2D((2, 2))(x)\n    x = Conv2D(3, (3, 3), activation='linear', padding='same')(x)\n    generated = Flatten()(x)\nelse:\n    seed = Input((size_discriminator,))\n    generated = Dense(img_size_flatten, activation='linear')(seed)\n\n# COMPILE\ngenerator = Model(seed, [generated,Reshape((size_discriminator,))(seed)])\n\n# DISPLAY ARCHITECTURE\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:27:08.023773Z","iopub.execute_input":"2022-08-11T19:27:08.024078Z","iopub.status.idle":"2022-08-11T19:27:08.120470Z","shell.execute_reply.started":"2022-08-11T19:27:08.024018Z","shell.execute_reply":"2022-08-11T19:27:08.119692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BUILD GENERATIVE ADVERSARIAL NETWORK\ndiscriminator.trainable=False    \ngan_input = Input(shape=(size_discriminator,))\nx = generator(gan_input)\ngan_output = discriminator(x)\n\n# COMPILE GAN\ngan = Model(gan_input, gan_output)\ngan.get_layer('model_1').get_layer('conv').set_weights([np.array([[[[-1 ]]],[[[255.]]]])])\ngan.compile(optimizer=Adam(5), loss='mean_squared_error')\n\n# DISPLAY ARCHITECTURE\ngan.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:28:02.821861Z","iopub.execute_input":"2022-08-11T19:28:02.822178Z","iopub.status.idle":"2022-08-11T19:28:03.125362Z","shell.execute_reply.started":"2022-08-11T19:28:02.822122Z","shell.execute_reply":"2022-08-11T19:28:03.124666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator Coaches Generator\nIn a typical GAN, the discriminator does not memorize the training images beforehand. Instead it learns to distinquish real images from fake images at the same time that the Generator learns to make fake images. In this GAN, we taught the Discriminator ahead of time and it will now teach the Generator.","metadata":{}},{"cell_type":"code","source":"# TRAINING DATA\ntrain = np.zeros((size_discriminator,size_discriminator))\nfor i in range(size_discriminator): train[i,i] = 1\nzeros = np.zeros((size_discriminator,img_size_flatten))\n\n# TRAIN NETWORKS\nep = 1; it = 9\nif BadMemory: lr = 0.005\nelse: lr = 5.\n    \nfor k in range(it):  \n\n    # BEGIN DISCRIMINATOR COACHES GENERATOR\n    annealer = LearningRateScheduler(lambda x: lr)\n    h = gan.fit(train, zeros, epochs = ep, batch_size=256, callbacks=[annealer], verbose=0)\n\n    # DISPLAY GENERATOR LEARNING PROGRESS \n    print('Epoch',(k+1),'/'+str(it)+' - loss =',h.history['loss'][-1] )\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        xx = np.zeros((size_discriminator))\n        xx[np.random.randint(size_discriminator)] = 1\n        plt.subplot(1,5,j+1)\n        img = generator.predict(xx.reshape((-1,size_discriminator)))[0].reshape((-1,img_size,img_size,3))\n        img = Image.fromarray( (img).astype('uint8').reshape((img_size,img_size,3)))\n        plt.axis('off')\n        plt.imshow(img)\n    plt.show()  \n            \n    # ADJUST LEARNING RATES\n    if BadMemory:\n        ep *= 2\n        if ep>=32: lr = 0.001\n        if ep>256: ep = 256\n    else:\n        if h.history['loss'][-1] < 25: lr = 1.\n        if h.history['loss'][-1] < 1.5: lr = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-08-11T19:28:09.003209Z","iopub.execute_input":"2022-08-11T19:28:09.003629Z","iopub.status.idle":"2022-08-11T20:00:05.449304Z","shell.execute_reply.started":"2022-08-11T19:28:09.003566Z","shell.execute_reply":"2022-08-11T20:00:05.448566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Generator Class\nOur Generative Network has now learned all the training images from our Discriminative Network. With its poor memory, we hope that it has learned to generalize somewhat. Now let's build a Generator Class that accepts any random 100 dimensional vector and outputs an image. Our class will return 70% of one \"memorized\" image mixed with 30% another. Since the images are stored in a convolutional network, we hope that it makes a generalized conceptual mixture (versus a pixel blend).","metadata":{}},{"cell_type":"code","source":"class DogGenerator:\n    index = 0   \n    def getDog(self,seed):\n        xx = np.zeros((size_discriminator))\n        xx[self.index] = 0.70\n        xx[np.random.randint(size_discriminator)] = 0.30\n        img = generator.predict(xx.reshape((-1,size_discriminator)))[0].reshape((img_size,img_size,3))\n        self.index = (self.index+1)%size_discriminator\n        return Image.fromarray( img.astype('uint8') ) ","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:02:17.294471Z","iopub.execute_input":"2022-08-11T20:02:17.294809Z","iopub.status.idle":"2022-08-11T20:02:17.301148Z","shell.execute_reply.started":"2022-08-11T20:02:17.294755Z","shell.execute_reply":"2022-08-11T20:02:17.300297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Examples of Generated Dogs","metadata":{}},{"cell_type":"code","source":"# DISPLAY EXAMPLE DOGS\nd = DogGenerator()\nfor k in range(3):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = d.getDog(np.random.normal(0,1,100))\n        plt.axis('off')\n        plt.imshow(img)\n    plt.show() ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-11T20:02:24.055042Z","iopub.execute_input":"2022-08-11T20:02:24.055357Z","iopub.status.idle":"2022-08-11T20:02:25.486288Z","shell.execute_reply.started":"2022-08-11T20:02:24.055299Z","shell.execute_reply":"2022-08-11T20:02:25.485537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit to Kaggle\nIn this kernel we learned how to make an experimental GAN. Currently it scores around LB 100. We must be careful as we try to improve its score. If we give this GAN excellent memory and request a mixture of 99.9% one image and 0.1% another, then it can score LB 7 but then it is returning \"altered versions\" of images and violates the rules [here][1]\n\n[1]: https://www.kaggle.com/c/generative-dog-images/discussion/98183","metadata":{}},{"cell_type":"code","source":"# SAVE TO ZIP FILE NAMED IMAGES.ZIP\nz = zipfile.PyZipFile('images.zip', mode='w')\nd = DogGenerator()\nfor k in range(size_discriminator):\n    img = d.getDog(np.random.normal(0,1,100))\n    f = str(k)+'.png'\n    img.save(f,'PNG'); z.write(f); os.remove(f)\n    #if k % 1000==0: print(k)\nz.close()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T20:02:35.428610Z","iopub.execute_input":"2022-08-11T20:02:35.428921Z","iopub.status.idle":"2022-08-11T20:02:57.843761Z","shell.execute_reply.started":"2022-08-11T20:02:35.428868Z","shell.execute_reply":"2022-08-11T20:02:57.842590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate LB Score\nIf you wish to compute LB, you must add the LB metric dataset [here][1] to this kernel and change the boolean variable in the first cell block.\n\n[1]: https://www.kaggle.com/wendykan/dog-face-generation-competition-kid-metric-input","metadata":{}},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\nimport numpy as np\nimport os\nimport gzip, pickle\nimport tensorflow as tf\nfrom scipy import linalg\nimport pathlib\nimport urllib\nimport warnings\nfrom tqdm import tqdm\nfrom PIL import Image\n\nclass KernelEvalException(Exception):\n    pass\n\nmodel_params = {\n    'Inception': {\n        'name': 'Inception', \n        'imsize': img_size,\n        'output_layer': 'Pretrained_Net/pool_3:0', \n        'input_layer': 'Pretrained_Net/ExpandDims:0',\n        'output_shape': 2048,\n        'cosine_distance_eps': 0.1\n        }\n}\n\ndef create_model_graph(pth):\n    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n    # Creates graph from saved graph_def.pb.\n    with tf.gfile.FastGFile( pth, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString( f.read())\n        _ = tf.import_graph_def( graph_def, name='Pretrained_Net')\n\ndef _get_model_layer(sess, model_name):\n    # layername = 'Pretrained_Net/final_layer/Mean:0'\n    layername = model_params[model_name]['output_layer']\n    layer = sess.graph.get_tensor_by_name(layername)\n    ops = layer.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n            if shape._dims != []:\n              shape = [s.value for s in shape]\n              new_shape = []\n              for j, s in enumerate(shape):\n                if s == 1 and j == 0:\n                  new_shape.append(None)\n                else:\n                  new_shape.append(s)\n              o.__dict__['_shape_val'] = tf.TensorShape(new_shape)\n    return layer\n\ndef get_activations(images, sess, model_name, batch_size=50, verbose=False):\n    \"\"\"Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n                     must lie between 0 and 256.\n    -- sess        : current session\n    -- batch_size  : the images numpy array is split into batches with batch size\n                     batch_size. A reasonable batch size depends on the disposable hardware.\n    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n                     batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, 2048) that contains the\n       activations of the given tensor when feeding inception with the query tensor.\n    \"\"\"\n    inception_layer = _get_model_layer(sess, model_name)\n    n_images = images.shape[0]\n    if batch_size > n_images:\n        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n        batch_size = n_images\n    n_batches = n_images//batch_size + 1\n    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n    for i in tqdm(range(n_batches)):\n        if verbose:\n            print(\"\\rPropagating batch %d/%d\" % (i+1, n_batches), end=\"\", flush=True)\n        start = i*batch_size\n        if start+batch_size < n_images:\n            end = start+batch_size\n        else:\n            end = n_images\n                    \n        batch = images[start:end]\n        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])\n    if verbose:\n        print(\" done\")\n    return pred_arr\n\n\n# def calculate_memorization_distance(features1, features2):\n#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n#     neigh.fit(features2) \n#     d, _ = neigh.kneighbors(features1, return_distance=True)\n#     print('d.shape=',d.shape)\n#     return np.mean(d)\n\ndef normalize_rows(x: np.ndarray):\n    \"\"\"\n    function that normalizes each row of the matrix x to have unit length.\n\n    Args:\n     ``x``: A numpy matrix of shape (n, m)\n\n    Returns:\n     ``x``: The normalized (by row) numpy matrix.\n    \"\"\"\n    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n\n\ndef cosine_distance(features1, features2):\n    # print('rows of zeros in features1 = ',sum(np.sum(features1, axis=1) == 0))\n    # print('rows of zeros in features2 = ',sum(np.sum(features2, axis=1) == 0))\n    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n    norm_f1 = normalize_rows(features1_nozero)\n    norm_f2 = normalize_rows(features2_nozero)\n\n    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n    print('d.shape=',d.shape)\n    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n    mean_min_d = np.mean(np.min(d, axis=1))\n    print('distance=',mean_min_d)\n    return mean_min_d\n\n\ndef distance_thresholding(d, eps):\n    if d < eps:\n        return d\n    else:\n        return 1\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n            \n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n             inception net ( like returned by the function 'get_predictions')\n             for generated samples.\n    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n               on an representive data set.\n    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n               generated samples.\n    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n               precalcualted on an representive data set.\n\n    Returns:\n    --   : The Frechet Distance.\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n\n    diff = mu1 - mu2\n\n    # product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n        warnings.warn(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        # covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n    \n    # numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(\"Imaginary component {}\".format(m))\n        covmean = covmean.real\n\n    # covmean = tf.linalg.sqrtm(tf.linalg.matmul(sigma1,sigma2))\n\n    print('covmean.shape=',covmean.shape)\n    # tr_covmean = tf.linalg.trace(covmean)\n\n    tr_covmean = np.trace(covmean)\n    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n    # return diff.dot(diff) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean\n#-------------------------------------------------------------------------------\n\n\ndef calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):\n    \"\"\"Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n                     must lie between 0 and 255.\n    -- sess        : current session\n    -- batch_size  : the images numpy array is split into batches with batch size\n                     batch_size. A reasonable batch size depends on the available hardware.\n    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n                     batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the incption model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the incption model.\n    \"\"\"\n    act = get_activations(images, sess, model_name, batch_size, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma, act\n    \ndef _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n    path = pathlib.Path(path)\n    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n    imsize = model_params[model_name]['imsize']\n\n    # In production we don't resize input images. This is just for demo purpose. \n    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n    m, s, features = calculate_activation_statistics(x, sess, model_name)\n    del x #clean up memory\n    return m, s, features\n\n# check for image size\ndef img_read_checks(filename, resize_to, is_checksize=False, check_imsize = img_size, is_check_png = False):\n    im = Image.open(str(filename))\n    if is_checksize and im.size != (check_imsize,check_imsize):\n        raise KernelEvalException('The images are not of size '+str(check_imsize))\n    \n    if is_check_png and im.format != 'PNG':\n        raise KernelEvalException('Only PNG images should be submitted.')\n\n    if resize_to is None:\n        return im\n    else:\n        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n\ndef calculate_kid_given_paths(paths, model_name, model_path, feature_path=None, mm=[], ss=[], ff=[]):\n    ''' Calculates the KID of two paths. '''\n    tf.reset_default_graph()\n    create_model_graph(str(model_path))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = True)\n        if len(mm) != 0:\n            m2 = mm\n            s2 = ss\n            features2 = ff\n        elif feature_path is None:\n            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n        else:\n            with np.load(feature_path) as f:\n                m2, s2, features2 = f['m'], f['s'], f['features']\n\n        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n        print('starting calculating FID')\n        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n        print('done with FID, starting distance calculation')\n        distance = cosine_distance(features1, features2)        \n        return fid_value, distance, m2, s2, features2","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-11T20:15:44.270906Z","iopub.execute_input":"2022-08-11T20:15:44.271220Z","iopub.status.idle":"2022-08-11T20:15:44.311194Z","shell.execute_reply.started":"2022-08-11T20:15:44.271164Z","shell.execute_reply":"2022-08-11T20:15:44.310455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ComputeLB:\n  \n    # UNCOMPRESS OUR IMGAES\n    with zipfile.ZipFile(\"../working/images.zip\",\"r\") as z:\n        z.extractall(\"../tmp/images2/\")\n\n    # COMPUTE LB SCORE\n    m2 = []; s2 =[]; f2 = []\n    user_images_unzipped_path = '../tmp/images2/'\n    images_path = [user_images_unzipped_path,'../input/generative-dog-images/all-dogs/all-dogs/']\n    public_path = '../input/dog-face-generation-competition-kid-metric-input/classify_image_graph_def.pb'\n\n    fid_epsilon = 10e-15\n\n    fid_value_public, distance_public, m2, s2, f2 = calculate_kid_given_paths(images_path, 'Inception', public_path, mm=m2, ss=s2, ff=f2)\n    distance_public = distance_thresholding(distance_public, model_params['Inception']['cosine_distance_eps'])\n    print(\"FID_public: \", fid_value_public, \"distance_public: \", distance_public, \"multiplied_public: \",\n            fid_value_public /(distance_public + fid_epsilon))\n    \n    # REMOVE FILES TO PREVENT KERNEL ERROR OF TOO MANY FILES\n    ! rm -r ../tmp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-11T20:15:48.099191Z","iopub.execute_input":"2022-08-11T20:15:48.099497Z","iopub.status.idle":"2022-08-11T20:16:31.617564Z","shell.execute_reply.started":"2022-08-11T20:15:48.099440Z","shell.execute_reply":"2022-08-11T20:16:31.616207Z"},"trusted":true},"execution_count":null,"outputs":[]}]}